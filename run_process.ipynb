{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775bf4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sub\n",
    "import subprocess as sp\n",
    "importlib.reload(sub) \n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "from contextlib import contextmanager\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    \"\"\"Temporarily redirect stdout and stderr to os.devnull.\"\"\"\n",
    "    devnull = open(os.devnull, 'w')\n",
    "    old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "    try:\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "        devnull.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95b0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sub = {}\n",
    "cwd = Path.cwd()\n",
    "sub_folder = cwd / \"derivates\"\n",
    "for folder_name in sub_folder.iterdir():\n",
    "    if folder_name.is_dir() and folder_name.name.startswith(\"sub_\"):\n",
    "        if folder_name.name == \"sub_025\":  # REMOVE SUB_25 he has no hypo data\n",
    "            continue\n",
    "        subject_name = folder_name.name\n",
    "        full_sub[subject_name] = sub.Subject(subject_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eacac0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funct_motion_correction():\n",
    "    for key, sub in tqdm(full_sub.items()):\n",
    "            print(f\"SUBJECT {key}\")\n",
    "            hyper = sub.data_hyper[\"func\"]\n",
    "            hypo = sub.data_hypo[\"func\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                    print(f\"Processing {key} {cond} for {run}\")\n",
    "                    func = condition[cond][run]\n",
    "                    func.reorient()\n",
    "                    func.motion_correction()\n",
    "                print(\"|\")\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "def func_epi_to_t1(sub_ids):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for key, sub in tqdm(selected_sub.items()):\n",
    "            hyper = sub.data_hyper[\"func\"]\n",
    "            hypo = sub.data_hypo[\"func\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                    print(f\"Processing {key} {cond} for {run}\")\n",
    "                    func = condition[cond][run]\n",
    "                    with suppress_stdout_stderr():\n",
    "                        func.func_to_t1()\n",
    "                print(\"|\")\n",
    "            print(\"-------------------------\")\n",
    "\n",
    "\n",
    "def func_t1_mask_to_epi(sub_ids):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for key, sub in tqdm(selected_sub.items()):\n",
    "            hyper = sub.data_hyper[\"func\"]\n",
    "            hypo = sub.data_hypo[\"func\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                    print(f\"Processing {key} {cond} for {run}\")\n",
    "                    func = condition[cond][run]\n",
    "                    with suppress_stdout_stderr():\n",
    "                        func.t1_mask_to_epi()\n",
    "                print(\"|\")\n",
    "            print(\"-------------------------\")\n",
    "\n",
    "def func_nuissance_regression(sub_ids):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for key, sub in tqdm(selected_sub.items()):\n",
    "            hyper = sub.data_hyper[\"func\"]\n",
    "            hypo = sub.data_hypo[\"func\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                    print(f\"Processing {key} {cond} for {run}\")\n",
    "                    func = condition[cond][run]\n",
    "                    with suppress_stdout_stderr():\n",
    "                        func.nuissance_regression(plot_design = False)\n",
    "                print(\"|\")\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "\n",
    "def func_epi_to_mni(sub_ids):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for key, sub in tqdm(selected_sub.items()):\n",
    "            hyper = sub.data_hyper[\"func\"]\n",
    "            hypo = sub.data_hypo[\"func\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                    print(f\"Processing {key} {cond} for {run}\")\n",
    "                    func = condition[cond][run]\n",
    "                    with suppress_stdout_stderr():\n",
    "                        func.epi_to_mni()\n",
    "                print(\"|\")\n",
    "            print(\"-------------------------\")\n",
    "            \n",
    "def anat_gm_mask_to_mni(sub_ids, th = 0.2):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for key, sub in tqdm(selected_sub.items()):\n",
    "            hyper = sub.data_hyper[\"anat\"]\n",
    "            hypo = sub.data_hypo[\"anat\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                #print(f\"Processing {key} {cond} anat\")\n",
    "                anat = condition[cond]\n",
    "                with suppress_stdout_stderr():\n",
    "                    anat.gm_mask_to_mni()\n",
    "                    anat.threshold_gm_mask(th=th)\n",
    "            #print(\"-------------------------\")\n",
    "\n",
    "\n",
    "def func_smoothing(sub_ids):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for key, sub in tqdm(selected_sub.items()):\n",
    "            hyper = sub.data_hyper[\"func\"]\n",
    "            hypo = sub.data_hypo[\"func\"]\n",
    "            condition = {\"hyper\": hyper, \"hypo\": hypo}\n",
    "            for cond in condition:\n",
    "                for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                    print(f\"Processing {key} {cond} for {run}\")\n",
    "                    func = condition[cond][run]\n",
    "                    with suppress_stdout_stderr():\n",
    "                        func.smoothing(fwhm=6)\n",
    "                print(\"|\")\n",
    "            print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef2531da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:19<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "anat_gm_mask_to_mni(full_sub, th=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f047937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_group_mask(subjs_ids, out_path= None, frac=0.6):\n",
    "    if out_path is None:\n",
    "        out_path = Path.cwd() / \"derivates\" / f\"group_mask_gm_th{frac*100:.0f}.nii.gz\"\n",
    "    subj_mask_paths = []\n",
    "    for sub in subjs_ids:\n",
    "        sub_gm_mask_hyper = full_sub[sub].data_hyper[\"anat\"].t1_gm_mask_thr\n",
    "        sub_gm_mask_hypo = full_sub[sub].data_hypo[\"anat\"].t1_gm_mask_thr\n",
    "        subj_mask_paths.append(sub_gm_mask_hyper)\n",
    "        subj_mask_paths.append(sub_gm_mask_hypo)\n",
    "    N = len(subj_mask_paths)\n",
    "    print(f\"Creating group mask from {N} subjects (threshold = {frac*100:.0f}%)\")\n",
    "\n",
    "    # Charger le premier masque comme référence\n",
    "    ref_img = nib.load(subj_mask_paths[0])\n",
    "    acc = np.zeros(ref_img.shape, dtype=np.int32)\n",
    "\n",
    "    # Additionner tous les masques binaires\n",
    "    for p in subj_mask_paths:\n",
    "        acc += (nib.load(p).get_fdata() > 0).astype(np.int32)\n",
    "\n",
    "    # Calcul du seuil (arrondi vers le haut)\n",
    "    thr = math.ceil(frac * N)\n",
    "\n",
    "    # Seuil et binarisation\n",
    "    grp = (acc >= thr).astype(np.uint8)\n",
    "\n",
    "    # Sauvegarde avec les mêmes affine/header\n",
    "    grp_img = nib.Nifti1Image(grp, ref_img.affine, ref_img.header)\n",
    "    grp_img.set_data_dtype(np.uint8)\n",
    "    nib.save(grp_img, out_path)\n",
    "\n",
    "    print(f\"[OK] Group mask saved to {out_path} (voxels kept = {int(grp.sum())})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39745ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating group mask from 34 subjects (threshold = 50%)\n",
      "[OK] Group mask saved to /Volumes/T7/MAP/derivates/group_mask_gm_th50.nii.gz (voxels kept = 147374)\n"
     ]
    }
   ],
   "source": [
    "make_group_mask(full_sub, frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2375710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, sub in full_sub.items():\n",
    "#     hyper = sub.data_hyper[\"func\"]\n",
    "#     hypo = sub.data_hypo[\"func\"]\n",
    "#     runs = [f\"R{i}\" for i in range(1,5)]\n",
    "#     for run in runs:\n",
    "#         # hyper_func = hyper[run].func_img_or_mc_bet_clean_withGSR_mni\n",
    "#         # hypo_func = hypo[run].func_img_or_mc_bet_clean_withGSR_mni\n",
    "#         hyper_func = hyper[run].func_img_or\n",
    "#         hypo_func = hypo[run].func_img_or\n",
    "#         hyper_img = nib.load(hyper_func)\n",
    "#         hypo_img = nib.load(hypo_func)\n",
    "#         hyper_data = hyper_img.get_fdata()\n",
    "#         hypo_data = hypo_img.get_fdata()\n",
    "#         print(f\"{id} {run} hyper: {hyper_data.shape} hypo: {hypo_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a43d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = cwd / \"derivates\"\n",
    "# for sub in folder.iterdir():\n",
    "#     if \"sub\" not in str(sub.name):\n",
    "#         continue\n",
    "#     if \"sub_\" in sub.name:\n",
    "#         func = sub / \"func\"\n",
    "#         func_hyper = func / \"hyper\"\n",
    "#         func_hypo = func / \"hypo\"\n",
    "#     for run in func_hyper.iterdir():\n",
    "#         no_gsr = 0 \n",
    "#         gsr = 0\n",
    "#         for run_file in run.iterdir():\n",
    "#             if \"clean_nogsr_mni_smooth\" in run_file.name:\n",
    "#                 no_gsr += 1\n",
    "#             if \"clean_withgsr_mni_smooth\" in run_file.name:\n",
    "#                 gsr += 1\n",
    "#         if gsr + no_gsr < 2:\n",
    "#             print(f\"Missing files in {run_file.parent}\")\n",
    "#         else :\n",
    "#             print(f\"Subject {sub.name} {run.name} hyper complete\")\n",
    "#     for run in func_hypo.iterdir():\n",
    "#         no_gsr = 0 \n",
    "#         gsr = 0\n",
    "#         for run_file in run.iterdir():\n",
    "#             if \"clean_nogsr\" in run_file.name:\n",
    "#                 no_gsr += 1\n",
    "#             if \"clean_withgsr\" in run_file.name:\n",
    "#                 gsr += 1\n",
    "#         if gsr + no_gsr < 2:\n",
    "#             print(f\"Missing files in {run_file.parent}\")\n",
    "#         else :\n",
    "#             print(f\"Subject {sub.name} {run.name} hypo complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a7fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 135.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_fmri_4d_to_3d(input_file, output_dir):\n",
    "    input_file = Path(input_file)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_prefix = output_dir / Path(input_file.stem).stem  \n",
    "    cmd = [\"fslsplit\", str(input_file), str(output_prefix) + \"_\", \"-t\"]\n",
    "    sp.run(cmd, check=True)\n",
    "\n",
    "\n",
    "def nii_gz_to_nii(input_file, output_dir = None):\n",
    "    input_file = Path(input_file)\n",
    "    if output_dir is None:\n",
    "        output_dir = input_file.parent\n",
    "    stem = Path(input_file.stem).stem       \n",
    "    output_name = f\"sw_{stem}.nii\"\n",
    "    output_file = output_dir / output_name\n",
    "    cmd = [\"fslchfiletype\", \"NIFTI\", str(input_file), str(output_file)]\n",
    "    sp.run(cmd, check=True)\n",
    "    return output_file\n",
    "\n",
    "\n",
    "\n",
    "def format_cap_analysis(sub_ids, gsr = True):\n",
    "    if type(sub_ids) is str:\n",
    "        sub_ids = [sub_ids]\n",
    "    selected_sub = {key: full_sub[key] for key in sub_ids}\n",
    "    for id, sub  in tqdm(selected_sub.items()):\n",
    "        for cond in [\"hyper\", \"hypo\"]:\n",
    "            for run in [f\"R{i}\" for i in range(1,5)]:\n",
    "                par_file = sub.data_hyper[\"func\"][run].func_img_or_mc_par if cond == \"hyper\" else sub.data_hypo[\"func\"][run].func_img_or_mc_par\n",
    "                if gsr : \n",
    "                    input_file = sub.data_hyper[\"func\"][run].func_img_or_mc_bet_clean_withGSR_mni_smooth if cond == \"hyper\" else sub.data_hypo[\"func\"][run].func_img_or_mc_bet_clean_withGSR_mni_smooth\n",
    "                    output_dir = Path.cwd() / \"cap_analysis_withgsr\" / f\"{id}_{cond}_{run}\" / \"func\"\n",
    "  \n",
    "                    #split_fmri_4d_to_3d(input_file, output_dir)\n",
    "                else :\n",
    "                    input_file = sub.data_hyper[\"func\"][run].func_img_or_mc_bet_clean_withoutGSR_mni_smooth if cond == \"hyper\" else sub.data_hypo[\"func\"][run].func_img_or_mc_bet_clean_withoutGSR_mni_smooth\n",
    "                    output_dir = Path.cwd() / \"cap_analysis_nogsr\" / f\"{id}_{cond}_{run}\" / \"func\"\n",
    "     \n",
    "                    #split_fmri_4d_to_3d(input_file, output_dir)\n",
    "                arr = np.loadtxt(par_file)        # Rx Ry Rz Tx Ty Tz\n",
    "                arr_cap = np.c_[arr[:, 3:6], arr[:, 0:3]]   # Tx Ty Tz Rx Ry Rz\n",
    "                np.savetxt(output_dir / \"motion_for_CAP.txt\", arr_cap, fmt=\"%.7g\")\n",
    "format_cap_analysis(full_sub, gsr = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee404b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sub.Subject at 0x313e8ba90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7246d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "392it [01:27,  4.46it/s]\n",
      "197it [00:54,  3.60it/s]\n",
      "197it [00:56,  3.48it/s]\n",
      "197it [00:57,  3.42it/s]\n",
      "197it [00:56,  3.49it/s]\n",
      "197it [00:54,  3.59it/s]\n",
      "197it [00:55,  3.56it/s]\n",
      "197it [00:54,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_sub1 = []\n",
    "for folder in Path(\"/Volumes/T7/MAP/cap_analysis_withgsr\").iterdir():\n",
    "    if \"sub_001\" in str(folder.name):\n",
    "        folder_sub1.append(Path(folder))\n",
    "\n",
    "for folder in folder_sub1:\n",
    "    func = folder / \"func\"\n",
    "    for file in tqdm(func.iterdir()):\n",
    "        if \"sub\" not in str(file.name):\n",
    "            continue\n",
    "        nii_gz_to_nii(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ea39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_to_nii(input_file, mask_file, output_file = None):\n",
    "    if output_file is None:\n",
    "        # we replace the inoput by the masked version\n",
    "        input_path = Path(input_file)\n",
    "        output_file = input_path\n",
    "    input_img = nib.load(input_file)\n",
    "    mask_img = nib.load(mask_file)\n",
    "    \n",
    "    input_data = input_img.get_fdata()\n",
    "    mask_data = mask_img.get_fdata()\n",
    "    \n",
    "    if input_data.shape != mask_data.shape:\n",
    "        raise ValueError(\"Input image and mask must have the same shape.\")\n",
    "    \n",
    "    masked_data = input_data * mask_data\n",
    "    \n",
    "    masked_img = nib.Nifti1Image(masked_data, input_img.affine, input_img.header)\n",
    "    nib.save(masked_img, output_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f24b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "392it [00:12, 32.22it/s]\n",
      "392it [00:13, 28.16it/s]\n",
      "392it [00:13, 28.98it/s]\n",
      "392it [00:13, 28.46it/s]\n",
      "392it [00:13, 28.44it/s]\n",
      "392it [00:13, 28.60it/s]\n",
      "392it [00:13, 29.07it/s]\n",
      "392it [00:13, 29.43it/s]\n"
     ]
    }
   ],
   "source": [
    "mask = \"/Volumes/T7/MAP/derivates/group_mask_gm_th60.nii.gz\"\n",
    "folder_sub1 = []\n",
    "for folder in Path(\"/Volumes/T7/MAP/cap_analysis_withgsr\").iterdir():\n",
    "    if \"sub_001\" in str(folder.name):\n",
    "        folder_sub1.append(Path(folder))\n",
    "\n",
    "for folder in folder_sub1:\n",
    "    func = folder / \"func\"\n",
    "    for file in tqdm(func.iterdir()):\n",
    "        if \"sub\" not in str(file.name):\n",
    "            continue\n",
    "        apply_mask_to_nii(file,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "506f39d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((91, 109, 91), (91, 109, 91))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "uno = nib.load(\"/Volumes/T7/MAP/cap_analysis_withgsr/sub_001_hyper_R2/func/sw_sub_001_HYPER_runR2_mc_bet_clean_withgsr_mni_smooth_0000.nii\")\n",
    "\n",
    "duo = nib.load(\"/Volumes/T7/MAP/cap_analysis_withgsr/sub_001_hyper_R1/func/sw_sub_001_HYPER_runR1_mc_bet_clean_withgsr_mni_smooth_0000.nii\")\n",
    "duo.get_fdata().shape, uno.get_fdata().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc3c4910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 109, 91)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNI_HEAD_2mm = Path(os.environ.get(\"FSLDIR\", \"/usr/local/fsl\") + \"/data/standard/MNI152_T1_2mm.nii.gz\")\n",
    "nib.load(MNI_HEAD_2mm).get_fdata().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745ff75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FD computed and saved to: /Volumes/T7/MAP/conf_FD.txt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "# Input and output paths\n",
    "func_mc = Path(\"/Volumes/T7/MAP/derivates/sub_002/func/hyper/runR1/mc/sub_002_HYPER_runR1_mc.nii.gz\")\n",
    "\n",
    "\n",
    "# Output filenames (will match FSL's convention)\n",
    "conf_file =  cwd / \"conf_FD.txt\"\n",
    "plot_file = cwd/ \"fd_plot2.png\"\n",
    "\n",
    "# Threshold (Power et al. FD > 0.5 mm is typical)\n",
    "threshold = 0.5\n",
    "\n",
    "# Build the command\n",
    "cmd = [\n",
    "    \"fsl_motion_outliers\",\n",
    "    \"-i\", str(func_mc),\n",
    "    \"-o\", str(conf_file),\n",
    "    \"--fd\",\n",
    "    f\"--thresh={threshold}\",\n",
    "    \"-p\", str(plot_file)\n",
    "]\n",
    "\n",
    "# Run it\n",
    "subprocess.run(cmd, check=True)\n",
    "print(\"✅ FD computed and saved to:\", conf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e3bd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('images/sub_017/anat/hypo/sub_017_HYPO_T1w.nii.gz')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Path(\"images/sub_017/anat/hypo\").glob(\"*.nii.gz\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_clustering(frames_buffer, k=8, random_state=42):\n",
    "#     print(f\"--- Running Clustering (k={k}) ---\")\n",
    "    \n",
    "#     # 1. Stack\n",
    "#     X_raw = np.vstack(frames_buffer)\n",
    "#     print(f\"Data shape: {X_raw.shape}\")\n",
    "    \n",
    "#     # --- CORRECTION FOR EXACT PAPER REPLICATION ---\n",
    "#     # The paper uses \"1 - Pearson's Correlation\".\n",
    "#     # Pearson requires Centering (Mean Removal) + Normalization.\n",
    "#     # Your previous code only did Normalization (Cosine).\n",
    "    \n",
    "#     # Step A: Subtract the Spatial Mean (Row-wise centering)\n",
    "#     X_centered = X_raw - X_raw.mean(axis=1, keepdims=True)\n",
    "    \n",
    "#     # Step B: L2 Normalization\n",
    "#     # Now, dot product of these vectors == Pearson Correlation\n",
    "#     X_norm = normalize(X_centered, axis=1, norm='l2')\n",
    "#     # ----------------------------------------------\n",
    "    \n",
    "#     # 3. K-Means\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "#     labels = kmeans.fit_predict(X_norm)\n",
    "    \n",
    "#     # Return X_raw (not centered) for map reconstruction, so maps show true intensity\n",
    "#     return labels, X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab571212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def find_optimal_k(frames_buffer, k_range=range(2, 21), random_state=42):\n",
    "#     \"\"\"\n",
    "#     Runs K-Means for a range of k values to help find the optimal number of clusters.\n",
    "#     Returns a DataFrame of metrics and the normalized data for final clustering.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Preprocessing Data (Once) ---\")\n",
    "    \n",
    "#     # 1. Stack\n",
    "#     X_raw = np.vstack(frames_buffer)\n",
    "    \n",
    "#     # 2. Centering (Spatial Mean Removal) - Crucial for Pearson Correlation\n",
    "#     X_centered = X_raw - X_raw.mean(axis=1, keepdims=True)\n",
    "    \n",
    "#     # 3. L2 Normalization - Crucial for Cosine/Pearson equivalence\n",
    "#     X_norm = normalize(X_centered, axis=1, norm='l2')\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     print(f\"--- Scanning k from {k_range.start} to {k_range.stop - 1} ---\")\n",
    "    \n",
    "#     for k in tqdm(k_range, desc=\"K-Means k values\", unit=\"k\"):\n",
    "#         # Run K-Means\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "#         labels = kmeans.fit_predict(X_norm)\n",
    "        \n",
    "#         # Metric A: Inertia (Sum of squared distances) -> For Elbow Method\n",
    "#         inertia = kmeans.inertia_\n",
    "        \n",
    "#         # Metric B: Silhouette Score (How distinct clusters are)\n",
    "#         # Note: This is computationally expensive. If >10k frames, consider subsampling.\n",
    "#         sil = silhouette_score(X_norm, labels)\n",
    "        \n",
    "#         results.append({\"k\": k, \"inertia\": inertia, \"silhouette\": sil})\n",
    "#         print(f\"k={k}: Inertia={inertia:.2f}, Silhouette={sil:.4f}\")\n",
    "        \n",
    "#     df_results = pd.DataFrame(results)\n",
    "    \n",
    "#     # --- Plotting the Results ---\n",
    "#     fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#     color = 'tab:blue'\n",
    "#     ax1.set_xlabel('Number of Clusters (k)')\n",
    "#     ax1.set_ylabel('Inertia (Lower is better)', color=color)\n",
    "#     ax1.plot(df_results['k'], df_results['inertia'], color=color, marker='o')\n",
    "#     ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#     ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "#     color = 'tab:red'\n",
    "#     ax2.set_ylabel('Silhouette Score (Higher is better)', color=color)\n",
    "#     ax2.plot(df_results['k'], df_results['silhouette'], color=color, marker='x', linestyle='--')\n",
    "#     ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#     plt.title(\"Optimization of K: Elbow Method vs Silhouette\")\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "    \n",
    "#     return df_results, X_norm, X_raw\n",
    "\n",
    "# # Usage Example:\n",
    "# # metrics_df, X_norm_ready, X_raw_ready = find_optimal_k(selected_frames_buffer)\n",
    "# # Look at the plot, choose your best K (e.g., 8), then run your final clustering:\n",
    "# # final_labels = KMeans(n_clusters=8, ...).fit_predict(X_norm_ready)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
